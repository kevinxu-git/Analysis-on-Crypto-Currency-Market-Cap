abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 24*(x^3 - x^2)/(1 - 5*x + 19*x^2 + 9*x^3)
}
plot(z, xlim = c(-4, 1), ylim = c(-2, 2), main = "Adams-Moulton 3-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 24*(x^3 - x^2)/(1 - 5*x + 19*x^2 + 9*x^3)
}
plot(z, xlim = c(-4, 1), ylim = c(-2, 2), main = "Adams-Moulton 3-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 720*(x^4 - x^3)/(-19 + 106*x - 264*x^2 + 646*x^3 + 251*x^4)
}
plot(z, xlim = c(-2, 1), ylim = c(-2, 2), main = "Adams-Moulton 4-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 720*(x^4 - x^3)/(-19 + 106*x - 264*x^2 + 646*x^3 + 251*x^4)
}
plot(z, xlim = c(-2, 1), ylim = c(-2, 2), main = "Adams-Moulton 4-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 720*(x^4 - x^3)/(-19 + 106*x - 264*x^2 + 646*x^3 + 251*x^4)
}
plot(z, xlim = c(-2, 1), ylim = c(-2, 2), main = "Adams-Moulton 4-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 3", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (Mod(1 + z + z^2/2 + z^3/6) < 1) {
points(i, j, pch = 20)
}
}
}
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 720*(x^4 - x^3)/(-19 + 106*x - 264*x^2 + 646*x^3 + 251*x^4)
}
plot(z, xlim = c(-2, 1), ylim = c(-2, 2), main = "Adams-Moulton 4-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 12*(x^2 - x)/(-1 + 8*x + 5*x^2)
}
plot(z, xlim = c(-6, 1), ylim = c(-4, 4), main = "Adams-Moulton 2-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 12*(x^2 - x)/(-1 + 8*x + 5*x^2)
}
plot(z, xlim = c(-5, 1), ylim = c(-4, 4), main = "Adams-Moulton 2-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 12*(x^2 - x)/(-1 + 8*x + 5*x^2)
}
plot(z, xlim = c(-5, 1), ylim = c(-4, 4), main = "Adams-Moulton 2-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 12*(x^2 - x)/(-1 + 8*x + 5*x^2)
}
plot(z, xlim = c(-5, 0), ylim = c(-4, 4), main = "Adams-Moulton 2-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 12*(x^2 - x)/(-1 + 8*x + 5*x^2)
}
plot(z, xlim = c(-5, 0), ylim = c(-4, 4), main = "Adams-Moulton 2-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 12*(x^2 - x)/(-1 + 8*x + 5*x^2)
}
plot(z, xlim = c(-5, 1), ylim = c(-4, 4), main = "Adams-Moulton 2-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 24*(x^3 - x^2)/(1 - 5*x + 19*x^2 + 9*x^3)
}
plot(z, xlim = c(-5, 1), ylim = c(-4, 4), main = "Adams-Moulton 3-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 720*(x^4 - x^3)/(-19 + 106*x - 264*x^2 + 646*x^3 + 251*x^4)
}
plot(z, xlim = c(-5, 1), ylim = c(-4, 4), main = "Adams-Moulton 4-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 12*(x^2 - x)/(-1 + 8*x + 5*x^2)
}
plot(z, xlim = c(-5, 1), ylim = c(-4, 4), main = "Adams-Moulton 2-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 24*(x^3 - x^2)/(1 - 5*x + 19*x^2 + 9*x^3)
}
plot(z, xlim = c(-5, 1), ylim = c(-4, 4), main = "Adams-Moulton 3-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 720*(x^4 - x^3)/(-19 + 106*x - 264*x^2 + 646*x^3 + 251*x^4)
}
plot(z, xlim = c(-5, 1), ylim = c(-4, 4), main = "Adams-Moulton 4-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 2", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (Mod(1 + z + z^2/2) = 1) {
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 2", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (Mod(1 + z + z^2/2) = 1) {
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 2", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (Mod(1 + z + z^2/2) = 1) {
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 2", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (Mod(1 + z + z^2/2) == 1) {
points(i, j, pch = 20)
}
}
}
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 2", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (abs(Mod(1 + z + z^2/2) - 1) < 0.1 {
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 2", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (abs(Mod(1 + z + z^2/2) - 1) < 0.1) {
points(i, j, pch = 20)
}
}
}
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 2", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (abs(Mod(1 + z + z^2/2) - 1) < 0.01) {
points(i, j, pch = 20)
}
}
}
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 2", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (abs(Mod(1 + z + z^2/2) - 1) < 0.001) {
points(i, j, pch = 20)
}
}
}
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 2", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (abs(Mod(1 + z + z^2/2) - 1) < 0.1 {
plot(0, 0, xlim = c(-3, 3), ylim = c(-3, 3), main = "Taylor series method of order 2", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
for (i in seq(-3, 3, 0.01)) {
for (j in seq(-3, 3, 0.01)) {
z <- complex(real = i, imaginary = j)
if (Mod(1 + z + z^2/2) < 1) {
points(i, j, pch = 20)
}
}
}
# contour(1:10, 20:29, matrix(seq(1:100), 10, 10))
n <- 100
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
z[j] <- exp(complex(real = 0, imaginary = theta[j])) - 1
}
plot(z, xlim = c(-3, 1), ylim = c(-2, 2), main = "Forward Euler", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
axis(side=1, at = (-3):1, labels=(-3):1)
# contour(theta, z)
# Color
# for (i in seq(-3, 1, 0.01)) {
#   for (j in seq(-2, 2, 0.01)) {
#     if (Mod(complex(real = i, imaginary = j) + 1) < 1) {
#       points(i, j, pch = 20)
#     }
#   }
# }
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
n <- 1000
theta <- seq(0, 2*pi, 2*pi/n)
z <- vector(mode = "numeric", length = n)
for ( j in 1:n) {
x <- exp(complex(real = 0, imaginary = theta[j]))
z[j] <- 720*(x^4 - x^3)/(-19 + 106*x - 264*x^2 + 646*x^3 + 251*x^4)
}
plot(z, xlim = c(-5, 1), ylim = c(-4, 4), main = "Adams-Moulton 4-step method", type = "l", xlab = "", ylab = "", asp = 1)
abline(h = 0)
abline(v = 0)
install.packages("uplift")
rm(list=ls())
# Linear Discriminant Analysis
library(DiscriMiner)
mowers.df <- read.csv("data/RidingMowers.csv")
setwd("~/Documents/Ecole_IngÃ©/2A/Yonsei/Courses/Data Mining/Project/DataMiningProject")
rm(list=ls())
data <- read.csv("data.csv")
n <- dim(data)[1]
#p is really need?
p <- dim(data)[2]
head(data)
dim(data)
summary(data)
t(t(names(data)))
data <- na.omit(data)
library(forecast)
market.ts <- ts(data[, c(2, 8)]$Market.Cap..., start = c(2015, 1), end = c(2019, 200), freq = 365)
plot(market.ts, xlab = "year", ylab = "Market.Cap (in $)")
data.pre <- data[,-c(1,2)]
data.summary <- data.frame(mean = sapply(data.pre, mean),
sd = sapply(data.pre, sd),
min = sapply(data.pre, min),
max = sapply(data.pre, max),
median = sapply(data.pre, median),
length = sapply(data.pre, length),
miss.val = sapply(data.pre, function(x)
sum(length(which(is.na(x))))))
round(cor(data.pre), 2)
pcs <- prcomp(data.pre, scale. = TRUE)
summary(pcs)
pcs$rot[,1:5]
plot(pcs)
library(factoextra)
fviz_eig(pcs)
fviz_pca_var(pcs,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
data.pre <- data[, -c(3, 10)]
data.pre <- data.pre[, -c(1, 2)]
set.seed(123)  # set seed for reproducing the partition
train.index <- sample(1:n, n*0.4)
valid.index <- sample(setdiff(1:n, train.index), n*0.4)
test.index <- setdiff(1:n, union(train.index, valid.index))
train.df <- data.pre[train.index, ]
valid.df <- data.pre[valid.index, ]
test.df <- data.pre[test.index, ]
## Linear model --> maybe we replae this linear regression model with hyeonjae's python code named LinearRegression.py
## But also this linear regression model maybe need gor the comparison to neural net in R, comparing MSE.
lm.full <- lm(Market.Cap... ~ ., data = train.df)
plot(lm.full)
library(forecast)
lm.full.pred <- predict(lm.full, valid.df)
accuracy(lm.full.pred, valid.df$Market.Cap...)
all.residuals <- valid.df$Market.Cap... - lm.full.pred
hist(all.residuals, breaks = 25, xlab = "Residuals", main = "")
library(gains)
gain <- gains(valid.df$Market.Cap..., lm.full.pred)
options(scipen=999)
plot(c(0, gain$cume.pct.of.total*sum(valid.df$Market.Cap...)) ~ c(0, gain$cume.obs),
xlab="# cases", ylab="Cumulative Market.Cap...", main="LiftChart",
type="l")
lines(c(0, sum(valid.df$Market.Cap...)) ~ c(0, dim(valid.df)[1]), col = "gray", lty = 2)
barplot(gain$mean.resp/mean(valid.df$Market.Cap...), names.arg = gain$depth,
xlab = "Percentile", ylab = "Mean Response",
main = "Decile-wise lift chart")
library(leaps)
search <- regsubsets(Market.Cap... ~ ., data = train.df, nbest = 1, nvmax = dim(train.df)[2], method = "exhaustive")
summary(search)
lm.full.step <- step(lm.full, direction = "both",intercept = FALSE)
summary(lm.full.step)
predictors <- c(1:4, 6)
train.norm.df <- train.df
valid.norm.df <- valid.df
test.norm.df <- test.df
library(caret)
norm.values <- preProcess(train.df[, predictors], method = c("center", "scale"))
train.norm.df[, predictors] <- predict(norm.values, train.df[, predictors])
valid.norm.df[, predictors] <- predict(norm.values, valid.df[, predictors])
test.norm.df[, predictors] <- predict(norm.values, test.df[, predictors])
library(class)
library(forecast)
kMax <- 10
accuracy.df <- data.frame(k = seq(1, kMax, 1), accuracy = rep(0, kMax))
for(i in 1:kMax) {
knn.pred <- class::knn(train.norm.df[, predictors], valid.norm.df[, predictors], cl = train.norm.df[, 5], k = i)
accuracy.df[i, 2] <- accuracy(as.numeric(knn.pred), valid.norm.df$Market.Cap...)[2]
}
k <- which.min(accuracy.df[,2])
k
accuracy.df[k,2]
knn.pred <- class::knn(train.norm.df[, predictors], valid.norm.df[, predictors], cl = train.norm.df[, 5], k = 3)
accuracy.df <- data.frame(k = seq(1, kMax, 1), accuracy = rep(0, kMax))
for(i in 1:kMax) {
knn.pred <- class::knn(train.norm.df[, predictors], valid.norm.df[, predictors], cl = train.norm.df[, 5], k = i)
accuracy.df[i, 2] <- accuracy(as.numeric(knn.pred), valid.norm.df$Market.Cap...)[2]
}
k <- which.min(accuracy.df[,2])
k
accuracy.df[k,2]
knn.pred <- class::knn(train.norm.df[, predictors], valid.norm.df[, predictors], cl = train.norm.df[, 5], k = 3)
knn.pred <- as.numeric(as.character(knn.pred))
accuracy(knn.pred, valid.norm.df$Market.Cap...)
par(mfrow=c(1,1))
plot(valid.df$Market.Cap..., knn.pred, xlab = "Real values", ylab = "Predicted value", col='red', main='Real vs predicted values - Validation Set - KNN', pch=18, cex=0.7)
valid.norm.df$Market.Cap... - valid.df$Market.Cap...
par(mfrow=c(1,1))
plot(valid.df$Market.Cap..., knn.pred, xlab = "Real values", ylab = "Predicted value", col='red', main='Real vs predicted values - Validation Set - KNN', pch=18, cex=0.7)
abline(0, 1, lwd=2)
gain <- gains(valid.df$Market.Cap..., knn.pred)
options(scipen=999)
plot(c(0, gain$cume.pct.of.total*sum(valid.df$Market.Cap...)) ~ c(0, gain$cume.obs),
xlab="# cases", ylab="Cumulative Market.Cap...", main="LiftChart - Validation Set - KNN",
type="l")
lines(c(0, sum(valid.df$Market.Cap...)) ~ c(0, dim(valid.df)[1]), col = "gray", lty = 2)
legend(250, 3000000000000, legend = c("Cumulative using predicted values",
"Cumulative using average"), col = c(1, "gray"), lty = c(1, 2))
barplot(gain$mean.resp/mean(valid.df$Market.Cap...), names.arg = gain$depth,
xlab = "Percentile", ylab = "Mean Response",
main = "Decile-wise lift chart - Validation Set - KNN")
library(neuralnet)
## KNN
### Data Normalization
predictors <- c(1:4, 6)
train.norm.df <- train.df
valid.norm.df <- valid.df
test.norm.df <- test.df
library(caret)
norm.values <- preProcess(train.df[, predictors], method = c("center", "scale"))
train.norm.df[, predictors] <- predict(norm.values, train.df[, predictors])
valid.norm.df[, predictors] <- predict(norm.values, valid.df[, predictors])
test.norm.df[, predictors] <- predict(norm.values, test.df[, predictors])
### Fit model
library(class)
library(forecast)
kMax <- 10
accuracy.df <- data.frame(k = seq(1, kMax, 1), accuracy = rep(0, kMax))
for(i in 1:kMax) {
knn.pred <- class::knn(train.norm.df[, predictors], valid.norm.df[, predictors], cl = train.norm.df[, 5], k = i)
accuracy.df[i, 2] <- accuracy(as.numeric(knn.pred), valid.norm.df$Market.Cap...)[2]
}
# accuracy.df
k <- which.min(accuracy.df[,2])
k
accuracy.df[k,2]
### Evaluating Predictive Performance of KNN
knn.pred <- class::knn(train.norm.df[, predictors], valid.norm.df[, predictors], cl = train.norm.df[, 5], k = 3)
knn.pred <- as.numeric(as.character(knn.pred))
accuracy(knn.pred, valid.norm.df$Market.Cap...)
#### Graph real vs predicted
par(mfrow=c(1,1))
### Evaluating Predictive Performance of KNN on test set
knn.pred <- class::knn(test.norm.df[, predictors], test.norm.df[, predictors], cl = test.norm.df[, 5], k = 3)
knn.pred <- as.numeric(as.character(knn.pred))
accuracy(knn.pred, test.norm.df$Market.Cap...)
#### Graph real vs predicted
par(mfrow=c(1,1))
plot(test.df$Market.Cap..., knn.pred, xlab = "Real values", ylab = "Predicted value", col='red', main='Real vs predicted values - Test Set - KNN', pch=18, cex=0.7)
abline(0, 1, lwd=2)
#### Graph real vs predicted
par(mfrow=c(1,1))
plot(test.df$Market.Cap..., knn.pred, xlab = "Real values", ylab = "Predicted value", col='red', main='Real vs predicted values - Test Set - KNN', pch=18, cex=0.7)
abline(0, 1, lwd=2)
#### Lift chart
gain <- gains(test.df$Market.Cap..., knn.pred)
options(scipen=999)
plot(c(0, gain$cume.pct.of.total*sum(test.df$Market.Cap...)) ~ c(0, gain$cume.obs),
xlab="# cases", ylab="Cumulative Market.Cap...", main="LiftChart - Test Set - KNN",
type="l")
lines(c(0, sum(test.df$Market.Cap...)) ~ c(0, dim(test.df)[1]), col = "gray", lty = 2)
legend(250, 3000000000000, legend = c("Cumulative using predicted values",
"Cumulative using average"), col = c(1, "gray"), lty = c(1, 2))
legend(150, 3000000000000, legend = c("Cumulative using predicted values",
"Cumulative using average"), col = c(1, "gray"), lty = c(1, 2))
#### Lift chart
gain <- gains(test.df$Market.Cap..., knn.pred)
options(scipen=999)
plot(c(0, gain$cume.pct.of.total*sum(test.df$Market.Cap...)) ~ c(0, gain$cume.obs),
xlab="# cases", ylab="Cumulative Market.Cap...", main="LiftChart - Test Set - KNN",
type="l")
lines(c(0, sum(test.df$Market.Cap...)) ~ c(0, dim(test.df)[1]), col = "gray", lty = 2)
legend(150, 2000000000000, legend = c("Cumulative using predicted values",
"Cumulative using average"), col = c(1, "gray"), lty = c(1, 2))
legend(150, 1500000000000, legend = c("Cumulative using predicted values",
"Cumulative using average"), col = c(1, "gray"), lty = c(1, 2))
legend(120, 1500000000000, legend = c("Cumulative using predicted values",
"Cumulative using average"), col = c(1, "gray"), lty = c(1, 2))
#### Lift chart
gain <- gains(test.df$Market.Cap..., knn.pred)
options(scipen=999)
plot(c(0, gain$cume.pct.of.total*sum(test.df$Market.Cap...)) ~ c(0, gain$cume.obs),
xlab="# cases", ylab="Cumulative Market.Cap...", main="LiftChart - Test Set - KNN",
type="l")
lines(c(0, sum(test.df$Market.Cap...)) ~ c(0, dim(test.df)[1]), col = "gray", lty = 2)
legend(120, 1500000000000, legend = c("Cumulative using predicted values",
"Cumulative using average"), col = c(1, "gray"), lty = c(1, 2))
#### Decile-wise lift chart
barplot(gain$mean.resp/mean(test.df$Market.Cap...), names.arg = gain$depth,
xlab = "Percentile", ylab = "Mean Response",
main = "Decile-wise lift chart - Test Set - KNN")
